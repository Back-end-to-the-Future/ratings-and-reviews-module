Week 9 Day 5:

I intend on making this a longer journal entry since I haven't been in the habit of keeping up with journaling every single day, object to object.

  Today, I am starting out my day with a little more pep in my step. I am more content than usual. I'm feeling this way because, happily, for the limited amount of work I chose to commit to doing, I actually completed it fantastically and on time. I've met all the deadlines and have kept up with the load I sought out to do at the beginning of this project. I feel stellar about that especially considering I was feeling literally the opposite emotions as now, just a week ago and even less. SDC was kicking my butt at the beginning and I allowed it to take effect on my emotions and my reality entirely. It's funny how all of that can change at the drop of a hat. I remember saying to myself and others that, "you know, I think I'm just not cut out for this", or, "you know, I think I'm just going to apply to front-end positions when I graduate! I like working the front-end and love React and even enjoy styling with CSS and/or a framework, but the SDC is hard!", and because of that belief, I was feeling unhappy with my SDC progress and output. I didn't realize it but sure enough I was makiing progress and it just appeared as if I weren't. Nature of the beast as it pertains the SDC. Overall though I'm grateful that I overcame that challenging slump down into pretty much full despair, because all of a sudden, now, I actually love this project and had a whole lot of fun doing it. Sure I didn't do every single last route that the this micro service requires, but I actually did do all of the ones that the legacy code base required, and just less than a week ago, that's a whole lot more than I believed I'd be able to accomplish.

  What have I been up to since the last time I did a journal entry, which was 6 days ago?

  Data generation:
    I have now refactored my data generation scripts from writeFile, which took a considerable amount of time, to createWriteStream. I have also implemented a drain event in each of my data generation files. When the highWaterMark is reached, the write method of createWriteStream will start returning false. The stream will internally queue a drain. Once the internal bufferâ€™s length is 0 then the drain event is fired. This basically allows me to be able to write a million or TEN MILLION records all in one go withot having to take a very long time to create records in seperate batches or even in seperate files. Using this awesome drain event method of which I have grown very fond of, I was able to generate 10 million records in a whopping 209 total seconds. Thats about 3 minutes and 48 seconds. Not too shabby considering just how many faker random data generation variables I actually had to generate ten million times.

    A huge take away in regards to faker word generation and time pressure in the generation of 10 million records is that, faker.random.words(n) take a significant amount more time than simply using fakers lorem ipsum. We're talking like a fifteen minute or more reduction in time taken to create that may records. That's a lot if you consider we're constantly crunched and challenged with time on this project. I will note that I personally like the look of faker.random.words better than lorem ipsum, but as for the progress of this project, I believe lorem is more than good enough. In the future if I ever have to do some sort of mass random data generation, I might go with the words, but for now, I'm content with what I've got!

    10 Million records in the reviews table time: 209 seconds
    5 Million records in the photos table time: 3 seconds
    ~2.5 Million records in the characteristics table time: 2 seconds

    Details on the characteristics script that I wrote. Coming up with the algorithm to create the data exactly how I wanted it to come out took some deep diving and thinking. I gave up on previous algorithms that didn't serve the purposes that I intended to accomplish. Even before I switched to createWriteStream and using a drain event, I had thought of a neat little algorithm that basically takes advantage of closure for both the characteristics_id and the product_id. They are declared outside of the loop that goes roughly 2.5 million times. The reason it's rough and not exact everytime is because if I were to run the file again to create those records, it would still be around 2.5 million records generated, but the exact number would be very different because i is set to million, so it's going to loop through exactly a million times which is exactly how mant product_id's will inherit characteristics, but there's a for loop inside of the do...while loop that loops a random number of times each loop within the do...while event. That inner for loop is what actually generates the characteristics and also keeps them incrementing, due to the closure scope of char_id being declared outside of the do...while loop, so that it perfectly represents the Greenfield API accurately, as far as id's are concerned. Now the names is another story entirely. I just decided to axe the idea of actually using the six characteristic names that came with the original Greenfield API and just use randomly generated names using faker. It's silly, it's fun, it's not perfect, but it get's the job done, and it perfectly serves the purpose of this SDC project, which is to generate the 10 million records and serve them up, with fast query times, deploy them, proxy them, make the query times go faster with more and more requests per second/query. I actually enjoy the unconventional naming and difference. Some simply but clever logic had to be imployed within my controller that handles the names that get served up in case of multiples, but I'll talk about that seperately.

  Model View Controller (MVC) structure:
    Yep, I did it. I seperated concerns. I abstracted. I essentially set this project's file structure up in what I believed to be the most professional way that one could do. Not only is this app now a PERN stack app, but it also employs Test Driven Development and MVC. I for one am thoroughly satisfied with that accomplishment. Not only that, but now I want to do every project I encounter like this. I feel so organized and professional and on top of that, to me, it helps me understand what is actually happening behind the scenes a lot more. Add to that, that it is actually more readable and understandable for me as well. Honestly I have no reason to not just do this every single time I create the back end to an application if it's up to me.

    I see how it all works behind the scenes, or underneath the hood as they say. Well, in the case for this project breaking my queries down into my model has been a blessing because honestly some of my queries have sub queries that have subqueries.

    Controllers are where my actual requests live. I've done it all with callbacks instead of promises, certainly not a decision I regret for it was something that A, got the job done, and B, got it done efficiently, and C, well I understand it in regards to MVC structure more than I do promises. I understand promises in general, and before this project, honestly it was all I used but, applying promises to the MVC structure was a little difficult starting out for me to extrapolate, so instead I just opted to do callbacks. And some callbacks have more callbacks within them, and yet another within that one. It's a fun and interesting little game to play, keeping track of all of them and making sure you send the data back out in the correct space. It took some fiddling, but at the end of the day, it works!

    Notedly, I also seperated my routes from the rest of my server. This is the first time I've abstracted that out, but it made sense come the time to do it, otherwise my server file would have become monolithic. Eased up the workflow and load of a single file and broke them into two pieces, a routes.js file and an index.js file which serves up my routes. It actually took some time to configure... like two whole hours of my life, haha, but that's okay because it finally works and it looks clean and more importantly it makes the file more readable and understandable. It's also another thing I can add to the resume and claim that I've done it now. It's more professional and it follows the MVC structure of abstaction and seperation of concerna quite nicely.

  Models:
    I mentioned it earlier, but in my model I have now, each route that I want served up, seperate files for them that allow for less code in the file and more legibility and understanding. Most notably, I want to discuss the reviews.js model. Holy Moly, if I were to say this model was hard to create and get working, that would be an understatement! The query for the reviews route is bananas. This is it: `SELECT review_id, rating, summary, recommend,
      COALESCE((response), '') AS response, body, date, reviewer_name, reviewer_email, helpfulness, reported,
      COALESCE((SELECT array_to_json(array_agg(row_to_json(photos)))
      FROM (SELECT id, url FROM photos WHERE reviews.review_id = photos.review_id) photos), '[]')
      AS photos
      FROM reviews
      WHERE product_id = ${product_id}`
      How ridiculous is that!? First off, I just want to say that I would have been intimidated to know that this is what I would have to accomplish creating if I saw this code before starting the query. Secondly, note to self, and I've said it once, I've said it many more times as well, and I'll say it again, take it slow. Take baby steps. Break the problem down into smaller problems that you can solve. With some great advice from Evan, I'll also remind myself to just get better at finding the mistake (not preventing it) and debugging through the code more mindfully and thoroughly. Second Evan advice, get better at looking things up that I have no idea how to do. Don't commit to code without a good idea and plan first and research for as long as you need to in order to discover what your objective is. Find it and then apply it. This is a hybrid of Teddi and Evan advice right here, get comfortable with the feeling of unknowing or not knowing. Be determined and believe in yourself. Have confidence that you will find the solution and do not surrender until you learn what you need to learn. ALSO, the fine line is, make sure your'e not too stuborn and prideful that you steamroll through all the signs that are telling you to move on and not get stuck in uneccessary rabbit holes. Timebox and recognize what is serving you or not in that moment. Anyways, that's a little bit of a tangent, but those are some undeniable nuggets of gold that needed to be flashed before the very eyes.

      What I learned specifically about the query itself is, not to include the (*) for the reviews table at the beginning of the query because, even if there's just one column that isn't necessary being in that result, it's worth just writing out all of the column names that you do need. However, having done that and said that just now, I'm willing to bet and discover that there are probably much easier and more efficient ways of returning the entire table minus the one column I dont need, but I'll leave that for a differnt time when that kind of query is necessary and I have, perhaps, more time and I can afford to contribute a little more effort towards that cause.

      COALESCE was an important and interesting lesson. It replaces the record that youre querying for with whatever you indicate you want it to be, instead of allowing it to be null. the main takeaway, put the quotes around whatever it is that you are choosing to replace null with. In the reviews query that I wrote in my model, for the photos table I chose to overwrite null with an empty array. I tried it for a long time without it working before realizing that it needed to be wrapped in quotes. So now it looks like this, '[]', which is the correct syntax.

      Aggregate functions. They're a little nuanced and challenging to explain and for me honestly, to fully undersand. Basically what I did was google how I wanted to massage the data, and then lo and behold, a Stack Overflow post comes in like a knight in shining armor to come and save my hide. Sure enough I trial and errors plenty of the queries that appeared within that post, but there was one that I finally typed in, and out pops a data structure damn near how I wanted it to appear

